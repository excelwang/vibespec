#!/usr/bin/env python3
"""
Vibe-Spec Smart Test Runner
Usage: python run_tests.py [tests_dir] [--spec SPEC_ID] [--generate]
"""
import argparse
import re
import subprocess
import sys
import json
import os
from pathlib import Path
from dataclasses import dataclass
from typing import List, Dict, Optional

# =============================================================================
# L3 Fixtures Parser (NEW)
# =============================================================================

@dataclass
class L3Item:
    """Parsed L3 item with type, ID, and fixtures."""
    item_type: str  # 'interface' | 'decision' | 'algorithm'
    item_id: str
    implements: Optional[str]
    fixtures: List[Dict[str, str]]
    raw_content: str


def parse_l3_fixtures(specs_dir: Path) -> List[L3Item]:
    """Parse L3-RUNTIME.md and extract all items with fixtures."""
    l3_file = specs_dir / 'L3-RUNTIME.md'
    if not l3_file.exists():
        return []
    
    content = l3_file.read_text()
    items = []
    
    # Split by ## [type] ID headers
    pattern = r'^## \[(interface|decision|algorithm)\] (\w+)'
    sections = re.split(pattern, content, flags=re.MULTILINE)
    
    # sections = [preamble, type1, id1, content1, type2, id2, content2, ...]
    for i in range(1, len(sections), 3):
        if i + 2 >= len(sections):
            break
        item_type = sections[i]
        item_id = sections[i + 1]
        section_content = sections[i + 2]
        
        # Extract Implements reference
        impl_match = re.search(r'Implements:\s*\[(?:Component|Role):\s*([^\]]+)\]', section_content)
        implements = impl_match.group(1).strip() if impl_match else None
        
        # Extract Fixtures table
        fixtures = parse_fixtures_table(section_content)
        
        items.append(L3Item(
            item_type=item_type,
            item_id=item_id,
            implements=implements,
            fixtures=fixtures,
            raw_content=section_content.strip()
        ))
    
    return items


def parse_fixtures_table(content: str) -> List[Dict[str, str]]:
    """Parse markdown table under **Fixtures**: header."""
    fixtures = []
    
    # Find Fixtures section
    match = re.search(r'\*\*Fixtures\*\*:\s*\n\|([^\n]+)\|\s*\n\|[-|\s]+\|\s*\n((?:\|[^\n]+\|\s*\n)*)', content)
    if not match:
        return fixtures
    
    # Parse header row
    header_line = match.group(1)
    headers = [h.strip() for h in header_line.split('|') if h.strip()]
    
    # Parse data rows
    data_section = match.group(2)
    for line in data_section.strip().split('\n'):
        if not line.strip():
            continue
        cells = [c.strip() for c in line.split('|') if c.strip()]
        if len(cells) >= len(headers):
            row = {headers[i]: cells[i] for i in range(len(headers))}
            fixtures.append(row)
    
    return fixtures


# =============================================================================
# Test Generator (NEW)
# =============================================================================

def generate_pytest_code(items: List[L3Item], output_dir: Path) -> List[Path]:
    """Generate tests from L3 fixtures, split by Role (YAML) and Component (Python)."""
    # Define targets
    acceptance_dir = output_dir.parent / 'specs' / 'acceptance'
    scripts_dir = output_dir.parent / 'specs' / 'scripts'
    
    acceptance_dir.mkdir(parents=True, exist_ok=True)
    scripts_dir.mkdir(parents=True, exist_ok=True)
    
    generated_files = []
    
    # Split items
    role_items = []
    component_items = []
    
    for item in items:
        # Heuristic: Check Implements tag or Item Type
        is_role = False
        if item.implements and ('Role:' in item.implements or 'ROLES.' in item.implements):
            is_role = True
        elif item.item_type == 'decision': # Fallback: Decisions are usually Roles
            is_role = True
            
        if is_role:
            role_items.append(item)
        else:
            component_items.append(item)
            
    # 1. Generate Acceptance Tests (YAML) for Roles
    for item in role_items:
        yaml_content = generate_yaml_test(item)
        path = acceptance_dir / f"{item.item_id.lower()}.yaml"
        path.write_text(yaml_content)
        generated_files.append(path)

    # 2. Generate Script Tests (Python) for Components
    # Group by type for cleaner files, or just one big file? 
    # Current pattern: test_interfaces.py, test_algorithms.py
    
    interfaces = [i for i in component_items if i.item_type == 'interface']
    algorithms = [i for i in component_items if i.item_type == 'algorithm']
    others = [i for i in component_items if i.item_type not in ['interface', 'algorithm']]
    
    if interfaces:
        code = generate_interface_tests(interfaces)
        path = scripts_dir / 'test_interfaces.py'
        path.write_text(code)
        generated_files.append(path)
        
    if algorithms:
        code = generate_algorithm_tests(algorithms)
        path = scripts_dir / 'test_algorithms.py'
        path.write_text(code)
        generated_files.append(path)

    if others:
        # Fallback for unexpected types
        code = generate_interface_tests(others) # Reuse interface generator
        path = scripts_dir / 'test_others.py'
        path.write_text(code)
        generated_files.append(path)
        
    return generated_files

def generate_yaml_test(item: L3Item) -> str:
    """Generate YAML test definition for Agent/Role."""
    lines = []
    lines.append(f"# Auto-generated from L3-RUNTIME.md [{item.item_type}] {item.item_id}")
    lines.append(f"# Implements: {item.implements}")
    lines.append("tests:")
    
    for i, fixture in enumerate(item.fixtures):
        input_val = fixture.get('Input', '').replace('"', '\\"')
        expected = fixture.get('Expected', '').replace('"', '\\"')
        reason = fixture.get('Reason', '')
        
        lines.append(f"  - id: {item.item_id}.case.{i+1}")
        lines.append(f"    description: \"{input_val} -> {expected}\"")
        lines.append(f"    input: \"{input_val}\"")
        lines.append(f"    expect: \"{expected}\"")
        if reason:
             lines.append(f"    reason: \"{reason}\"")
    
    return '\n'.join(lines)


def generate_interface_tests(items: List[L3Item]) -> str:
    """Generate pytest tests for interface fixtures."""
    lines = [
        '#!/usr/bin/env python3',
        '"""Auto-generated from L3-RUNTIME.md [interface] items."""',
        '"""Auto-generated from L3-RUNTIME.md [interface] items."""',
        'import unittest',
        '',
        '# TODO: Import actual implementations',
        '# from vibe_spec.scanner import Scanner',
        '',
        '# Dummy decorator for test discovery',
        'def verify_spec(id):',
        '    return lambda f: f',
        '',
    ]
    
    for item in items:
        lines.append(f'@verify_spec("{item.item_id}")')
        lines.append(f'class Test{item.item_id.title()}(unittest.TestCase):')
        lines.append(f'    """Tests for {item.item_id} interface."""')
        lines.append(f'    # Implements: {item.implements}')
        lines.append('')
        
        for i, fixture in enumerate(item.fixtures):
            input_val = fixture.get('Input', '')
            expected = fixture.get('Expected', '')
            case_type = fixture.get('Case', 'normal')
            
            test_name = f'test_{case_type.lower()}_{i+1}'
            lines.append(f'    def {test_name}(self):')
            lines.append(f'        """Input: {input_val}, Expected: {expected}"""')
            lines.append(f'        # TODO: Implement actual test')
            lines.append(f'        self.skipTest("Not implemented")')
            lines.append('')
        
        lines.append('')
    
    return '\n'.join(lines)


def generate_algorithm_tests(items: List[L3Item]) -> str:
    """Generate pytest tests for algorithm fixtures."""
    lines = [
        '#!/usr/bin/env python3',
        '"""Auto-generated from L3-RUNTIME.md [algorithm] items."""',
        'import unittest',
        '',
        '# Dummy decorator for test discovery',
        'def verify_spec(id):',
        '    return lambda f: f',
        '',
    ]
    
    for item in items:
        lines.append(f'@verify_spec("{item.item_id}")')
        lines.append(f'class Test{item.item_id.title().replace("_", "")}(unittest.TestCase):')
        lines.append(f'    """Tests for {item.item_id} algorithm."""')
        lines.append(f'    # Implements: {item.implements}')
        lines.append('')
        
        for i, fixture in enumerate(item.fixtures):
            input_val = fixture.get('Input', '')
            expected = fixture.get('Expected', '')
            case_type = fixture.get('Case', 'normal')
            
            test_name = f'test_{case_type.lower()}_{i+1}'
            lines.append(f'    def {test_name}(self):')
            lines.append(f'        """Input: {input_val}, Expected: {expected}"""')
            lines.append(f'        self.skipTest("Not implemented")')
            lines.append('')
        
        lines.append('')
    
    return '\n'.join(lines)


def generate_decision_stubs(items: List[L3Item]) -> str:
    """Generate LLM verification stubs for decision fixtures."""
    lines = [
        '#!/usr/bin/env python3',
        '"""',
        'Auto-generated from L3-RUNTIME.md [decision] items.',
        'These tests require LLM verification - run with --llm flag.',
        '"""',
        'import unittest',
        '',
        '# pytestmark = pytest.mark.llm  # Mark all as LLM tests',
        '',
        '# Dummy decorator for test discovery',
        'def verify_spec(id):',
        '    return lambda f: f',
        '',
    ]
    
    for item in items:
        lines.append(f'@verify_spec("{item.item_id}")')
        lines.append(f'class Test{item.item_id.title().replace("_", "")}(unittest.TestCase):')
        lines.append(f'    """LLM verification for {item.item_id} decision."""')
        lines.append(f'    # Implements: {item.implements}')
        lines.append('')
        
        for i, fixture in enumerate(item.fixtures):
            input_val = fixture.get('Input', '')
            expected = fixture.get('Expected', '')
            reason = fixture.get('Reason', '')
            
            test_name = f'test_fixture_{i+1}'
            lines.append(f'    def {test_name}(self):')
            lines.append(f'        """')
            lines.append(f'        Input: {input_val}')
            lines.append(f'        Expected: {expected}')
            lines.append(f'        Reason: {reason}')
            lines.append(f'        """')
            lines.append(f'        self.skipTest("Requires LLM verification")')
            lines.append('')
        
        lines.append('')
    
    return '\n'.join(lines)


# =============================================================================
# Existing Functions (preserved)
# =============================================================================

def find_verified_tests(tests_dir: Path, spec_filter: str = None) -> dict:
    """Find all test files with @verify_spec decorators."""
    tests = {}
    patterns = ['test_*.py', '*_test.py', '*.test.js', '*.spec.js', '*.test.ts', '*.spec.ts', '*_test.go']
    
    for pattern in patterns:
        for test_file in tests_dir.rglob(pattern):
            try:
                content = test_file.read_text()
                matches = re.findall(r'@verify_spec\s*\(\s*["\']([A-Z0-9_.]+)["\']\s*\)', content)
                if matches:
                    if spec_filter:
                        filtered = [m for m in matches if spec_filter in m]
                        if filtered:
                            tests[test_file] = filtered
                    else:
                        tests[test_file] = matches
            except Exception:
                pass
    
    return tests


def detect_framework(project_root: Path) -> tuple:
    """Detect project language and appropriate test framework."""
    if (project_root / "pytest.ini").exists() or (project_root / "conftest.py").exists() or \
       (project_root / "requirements.txt").exists() or (project_root / "pyproject.toml").exists():
        return "python", [sys.executable, "-m", "pytest", "-v"]

    if (project_root / "package.json").exists():
        try:
            pkg = json.loads((project_root / "package.json").read_text())
            deps = {**pkg.get("dependencies", {}), **pkg.get("devDependencies", {})}
            if "vitest" in deps:
                return "javascript", ["npx", "vitest", "run"]
            if "jest" in deps:
                return "javascript", ["npx", "jest"]
            return "javascript", ["npm", "test"]
        except Exception:
            return "javascript", ["npm", "test"]

    if (project_root / "go.mod").exists() or list(project_root.glob("*_test.go")):
        return "go", ["go", "test", "./..."]

    # Default to unittest (StdLib)
    return "python", [sys.executable, "-m", "unittest", "-v"]



class TestRunner:
    """Implements TEST_RUNNER component logic."""
    
    def run(self, tests_dir: Path, env: str = 'REAL') -> int:
        """
        Execute tests in specified environment.
        
        Args:
            tests_dir: Path to tests directory
            env: 'MOCK' | 'REAL' (default: REAL)
            
        Returns:
            int: Exit code (0=Success, 1=Failure)
        """
        if not tests_dir.exists():
            print(f"Tests root not found: {tests_dir}")
            return 1
            
        # 1. Discovery
        tests = find_verified_tests(tests_dir)
        if not tests:
            print("No tests found")
            return 0
            
        # 2. Environment Toggle
        if env == 'MOCK':
            print("üé≠ Running in MOCK environment (Simulation Mode)...")
            # In a real system, this would inject mock configs or set env vars
            # For now, we simulate success
            return 0
            
        # 3. Real Execution
        project_root = Path.cwd()
        test_files = list(tests.keys())
        
        lang, cmd_base = detect_framework(project_root)
        if lang == "unknown":
            print("‚ö†Ô∏è  Could not detect test framework")
            return 1
        
        cmd = cmd_base + [str(f) for f in test_files]
        print(f"üöÄ Running {lang} tests...")
        return subprocess.call(cmd)

def run_script_tests(test_files: list, project_root: Path) -> int:
    # Deprecated wrapper for backward compatibility
    runner = TestRunner()
    # Note: Traditional run_script_tests didn't take an env, assuming REAL
    # We reconstruct the behavior by calling subprocess directly or using logic
    # But since we are replacing logic, let's just make this wrap the new class
    # However, signature doesn't match perfectly. Let's keep the old logic for now or update callers
    # Given the script structure, let's just update main() to use TestRunner
    pass 



def find_l3_specs(specs_dir: Path) -> set:
    """Extract all L3 spec IDs from L3-*.md files (updated for new format)."""
    l3_items = set()
    for spec_file in specs_dir.glob('L3-*.md'):
        try:
            content = spec_file.read_text()
            # Match ## [type] ID headers
            matches = re.findall(r'^## \[(interface|decision|algorithm)\] (\w+)', content, re.MULTILINE)
            for _, item_id in matches:
                l3_items.add(item_id)
        except Exception:
            pass
    return l3_items


def check_coverage(specs_dir: Path, tests_dir: Path) -> tuple:
    """Check coverage and return (covered, uncovered) sets."""
    l3_specs = find_l3_specs(specs_dir)
    verified = set()
    
    for test_file in tests_dir.rglob('*'):
        if test_file.is_file():
            try:
                content = test_file.read_text()
                matches = re.findall(r'@verify_spec\s*\(\s*["\']([A-Z0-9_.]+)["\']\s*\)', content)
                verified.update(matches)
            except Exception:
                pass
    
    covered = set()
    for spec in l3_specs:
        for v in verified:
            if spec == v or v.startswith(spec + ".") or spec in v:
                covered.add(spec)
                break
    
    uncovered = l3_specs - covered
    return covered, uncovered


# =============================================================================
# Main
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description='Vibe-Spec Smart Test Runner',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument('tests_root', nargs='?', default='./tests',
                        help='Root directory for tests (default: ./tests)')
    parser.add_argument('--specs-dir', default='./specs',
                        help='Directory containing spec files (default: ./specs)')
    parser.add_argument('--spec', '-s', metavar='SPEC_ID',
                        help='Filter tests by spec ID')
    parser.add_argument('--list', '-l', action='store_true',
                        help='List tests without running')
    parser.add_argument('--coverage', '-c', action='store_true',
                        help='Show coverage report only')
    parser.add_argument('--generate', '-g', action='store_true',
                        help='Generate tests from L3 fixtures')
    parser.add_argument('--output', '-o', default='./tests/generated',
                        help='Output directory for generated tests')
    args = parser.parse_args()
    
    tests_root = Path(args.tests_root)
    specs_dir = Path(args.specs_dir)
    
    # Generate mode
    if args.generate:
        print("üìù Parsing L3 fixtures...")
        items = parse_l3_fixtures(specs_dir)
        if not items:
            print("No L3 items found in specs/L3-RUNTIME.md")
            return 1
        
        print(f"   Found {len(items)} L3 items:")
        for item in items:
            print(f"     [{item.item_type}] {item.item_id} ({len(item.fixtures)} fixtures)")
        
        print(f"\nüîß Generating tests to {args.output}...")
        generated = generate_pytest_code(items, Path(args.output))
        for f in generated:
            print(f"   ‚úì {f}")
        
        print(f"\n‚úÖ Generated {len(generated)} test files")
        return 0
    
    # List mode
    if args.list:
        tests = find_verified_tests(tests_root, args.spec)
        print(f"Found {len(tests)} test files with @verify_spec:\n")
        for test_file, spec_ids in tests.items():
            print(f"  {test_file.relative_to(tests_root)}")
            for sid in spec_ids:
                print(f"    ‚Üí {sid}")
        return 0
    
    # Coverage mode
    if args.coverage:
        covered, uncovered = check_coverage(specs_dir, tests_root)
        total = len(covered) + len(uncovered)
        pct = (len(covered) / total * 100) if total > 0 else 100
        print(f"Coverage: {pct:.0f}% ({len(covered)}/{total} L3 items)")
        if uncovered:
            print(f"\n‚ö†Ô∏è  Uncovered specs ({len(uncovered)}):")
            for s in sorted(uncovered)[:10]:
                print(f"    {s}")
            if len(uncovered) > 10:
                print(f"    ... and {len(uncovered) - 10} more")
        return 0 if not uncovered else 1
    
    # Default: run tests
    # Detect environment from env var (as per L1 contract)
    env = os.environ.get('TEST_ENV', 'REAL')
    
    runner = TestRunner()
    exit_code = runner.run(tests_root, env)
    
    # Coverage report
    if specs_dir.exists():
        covered, uncovered = check_coverage(specs_dir, tests_root)
        total = len(covered) + len(uncovered)
        if total > 0 and uncovered:
            pct = (len(covered) / total * 100)
            print(f"\n‚ö†Ô∏è  Coverage: {pct:.0f}% ({len(covered)}/{total})")
    
    return exit_code


if __name__ == "__main__":
    sys.exit(main())
